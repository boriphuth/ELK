In this section, I will explain how to install and set-up Filebeat on Centos/RedHat.
# Download
Filebeat can be installed as a service otherwise you have to download the zip folder to extract (this solution could be useful to discover the tool or if you only have windows installed on your machine but not for a stable solution).  
For the rest of this document, we use version 7.3 of filebeat, which can be downloaded at https://www.elastic.co/downloads/past-releases/filebeat-7-3-0  
Choose the version that corresponds to your operating system.
## The key folders  <_linux_OS_>
There is 3 interestion paths to know:
- **/var/log/filebeat:** In this folder you can find logs generated by filebeat. This folder is useful for a debugging need
- **/var/lib/filebeat:** Filebeat stores data in relation to messages sent to your receiver
- **/etc/filebeat:** In this folder, you can find the configuration file "filebeat.yml"
# Configuration file

There are two interesting parts to configure to run the filebeat agent. 
- Input section where you can specify one or more inputs. https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html
- Output section where you have to choose the receiver. You can choose one receiver between Kafka, Logstash, Kibana, Elasticsearch... https://www.elastic.co/guide/en/beats/filebeat/current/configuring-output.html 

## Input configuration
In this example, we imagine having csv files and log files in two different folders: `/home/user/csv_files/` & `/home/user/log_files/`  
We have to edit the filebeat yaml file (/etc/filebeat/filebeat.yml) and define two inputs.
```
filebeat.inputs:
- type: log
  enabled: true 
  paths:
    - /home/user/log_files/*.log
- type: log
  enabled: true
  paths:
    - /home/user/cvs_files/*.csv
````
* To enable an input we set `enabled` to true.  
  
Other parameters allow you to improve the performance of your filebeat agent and thus avoid the risk of data loss. Filebeat uses default settings, but you need to change them to suit your use case.   
To understand how the management of sent files works, we will take a `test.log file`, filebeat will send the data from this file to your receiver, but what happens when you modify your file, you have to send everything back?  
Filebeat uses a queue to store information about sent files, and keeps the file opened for 5 minutes. If no changes are made during this timeframe the file is closed.  
There are two common use cases:  
* **Use case 1:** You receive several files in a regular time interval and your files are not editable.
* **Use case 2:** You have some files but they're still writable. (for example a log file and every minute you have new lines)
  
### Use case 1
For this use case, keep files opened for 5 minutes can led to data loss. We imagine receiving hundreds of files every minute, you will reduce the performance of your filebeat agent if it has to keep the files in queue. And you know that your files are not modifiable, the default time of 5 minutes is not necessary in this case and the best way is to force filebeat to close each file once the transfer is finished.  
The field to add to your input section is `close_eof`. When this option is enabled, Filebeat closes a file as soon as the end of a file is reached.
```
filebeat.inputs:
- type: log
  enabled: true 
  paths:
    - /home/user/log_files/*.log
  close_eof: true
  ```
### Use case 2
For this use case, you can keep the file opened and wait for changes. The 5 minute period could be short if you know that new lines are added to your file every 10 minutes.  
The field to add to your input section is `close_inactive`. When this option is enabled, Filebeat closes a file, if no changes have been made, after the specific duration.  
```
filebeat.inputs:
- type: log
  enabled: true 
  paths:
    - /home/user/log_files/*.log
  close_inactive: 10m
  ```
For both use cases, we can add other fields such as how often we scan new files `scan_frequency`, ignore old files `ignore_older`, clean the filebeat registry after a period of time `clean_inactive`...  

# Output configuration
you have to choose an output among:
## Elasticsearch output

```
output.elasticsearch:
  hosts: ["elastic_ip:9200"]
  #username: "elastic"
  #password: "elastic"

```
You have to specify the elasticsearch ip address and user/pwd if authentification needed to access to your elastic.

## Logstash output

```
#output.logstash:
  # The Logstash hosts
  #hosts: ["logstash_ip:5044"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

```
You have to specify the logstash ip address and ssl client authentication if required.

## Kafka output
```
output.kafka:
  # initial brokers for reading cluster metadata
  hosts: ["kafka_ip:9092"]

  # message topic selection + partitioning
  topic: 'your_topic_name'
```

I kept this ouput at the end because it's the most difficult to configure but it can save you if you have several outputs to define. 
To make it simple, let's imagine our starting case, 2 directories, one with log files and another one with CSV files. You'll probably need to send these files to two different destinations, for example the destination is logstash which parses the data, but filebeat allows you to define only one logstash output. So you'll need to put two filebeat agents ?  
Kafka is the answer to this problem. As mentioned in the description of kafka, it is a powerful tool that allows you to collect data from several sources, so you can process the data you are interested in by choosing the right topic.  
In the example cited in the input section, we will add two fields that allow us to define a topic for each type of file

```
filebeat.inputs:
- type: log
  enabled: true 
  paths:
    - /home/user/log_files/*.log
  fields:
    name: log-topic
- type: log
  enabled: true
  paths:
    - /home/user/cvs_files/*.csv
  fields:
    name: csv-topic
````

  
Now we have two different topics `log-topic` and `csv-topic`. We can then configure kafka as follows to send each input to the right topic
```
output.kafka:
  # initial brokers for reading cluster metadata
  hosts: ["kafka_ip:9092"]

  # message topic selection + partitioning
  topic: '%{[fields.name]}'
```

* For the three output configurations, we can choose one or more hosts. for example `hosts: ["kafka_ip_1:9092","kafka_ip_2:9092","kafka_ip_3:9092"]`
