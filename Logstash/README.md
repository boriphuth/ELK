In this section, I will explain how to install and set-up logstash on Centos/RedHat.
# Download
Logstash can be installed as a service otherwise you have to download the zip folder to extract (this solution could be useful to discover the tool or if you only have windows installed on your machine but not for a stable solution).  
For the rest of this document, we use version 7.3 of Elasticsearch, which can be downloaded at https://www.elastic.co/downloads/past-releases/logstash-7-3-0  
Choose the version that corresponds to your operating system.

# Prerequisites
- CentOs server and a user with sudo privileges.
- Large memory capacity, at least 4 GB of RAM
- OpenJDK installed

# Setup Elastic instances
It is necessary to configure the machine where you have installed the Logstash module. The first thing to do is to check your java version, a new version of java 8 is recommended. Then you have to increase the maximum number of threads maintained by the Java virtual machine: 

```
$ sudo sysctl -w vm.max_map_count=262144
```
# The key folders
There is 3 interestion paths to know:
- **/var/log/logstash:** In this folder you can find logs generated by elasticsearch. This folder is useful for a debugging need
- **/var/lib/logstash:** elasticsearch stores data in this folder.
- **/etc/logstash:** In this folder, you can find the configuration files. 

# Configure Logstash
Open the /etc/logstash/jvm.options file and increase allocates memory to your logstash. Depending on your server, you can choose for example 2 GB if you have a server with 8 GB of Ram

```
# Xms represents the initial size of total heap space
# Xmx represents the maximum size of total heap space

-Xms2g
-Xmx2g

```

Open the /etc/elasticsearch/logstash.yml file and specify the number of workers. If you have a server with 8 core processor you can set the maximum of workers to 8

```
# Set the number of workers that will, in parallel, execute the filters+outputs
# stage of the pipeline.
#
# This defaults to the number of the host's CPU cores.
#
pipeline.workers: 8
```
# Create a Logstash config file 
The config file is used to parse data, you have to specify the input (where data are located), the filter 

```
input {
  ...
}

filter {
  ...
}

output {
  ...
}
```
## Input configuration
An input plugin allows Logstash to read a specific event source: file, kafka, beat... https://www.elastic.co/guide/en/logstash/current/input-plugins.html  

For this example, we'll configure logstash to parse the data from the `csv-topis` queued on kafka.

Create a new file `logstash.conf`

```
$ sudo vi /etc/logstash/conf.d/logstash.conf
```

Add the input plugin section:
```
input {
        kafka {
        bootstrap_servers => "server_kafka1:9092,server_kafka2:9092,server_kafka3:9092"
        group_id => "central-logging"
        topics =>  ["csv-topic"]
    }

}
```
* You can set a unique `bootstrap_servers` if you don't have a kafka cluster
## Filter configuration
This section used to process events, Logstash allows to process several plugins: json, csv, xml...https://www.elastic.co/guide/en/logstash/current/filter-plugins.html   
If you used kafka in the input part to retrieve the data, it will be stored in json format. And your initial data queued by kafka will be in the `message` field

```
filter{

        json {
            source => "message" target=> "theCSV"
        }
```
* We give a target `theCSV` to the initial data in order to be able to continue the processing of this data. For example, if our data is in csv format we can continue to parse the data like this:
```
filter{

        json {
            source => "message" target=> "theCSV"
        }
        csv {
            source => "[theCSV][message]"
            separator => ";"
      }
```

## Output configuration

An output plugin sends event data to a particular destination, for example elasticsearch:
```
output {
   elasticsearch {
     hosts => "http://elastic_ip:9200"
     index => "index_name"
   }
   }
```
* We can set one or more outputs if we have an elasticsearch cluster `hosts => ["es1:9200","es2:9200","es3:9200"]"`
* We can set a different index for each file, for example for csv-files, we define an index 'csv-files' `index => "csv-files"`

# Start logstash
```
$ sudo systemctl start logstash
```

